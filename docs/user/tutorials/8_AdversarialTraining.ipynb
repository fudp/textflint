{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8064b763",
   "metadata": {},
   "source": [
    "# Adversarial Training\n",
    "In recent years, people have found that even small perturbations can lead models make false prediction, which means the robustness of model is not good. How to improve the robustness of the model becomes a problem, Adversarial training is a method to improve the robustness of models. \n",
    "\n",
    "In this notebook, we will show you the model's not robust under the attack produced by textflint. And how adversarial training can help improve the robustness of the same model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba1bff06",
   "metadata": {},
   "source": [
    "## Prepare Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf1ce497",
   "metadata": {},
   "source": [
    "First, you need to prepare data sets for training and testing. Here we use 5000 pieces of data for training and 500 pieces of data for testing, all data are from the IMDB data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4c10aa3d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34;1mTextFlint\u001b[0m: ******Start load!******\n",
      "100%|██████████| 5000/5000 [00:11<00:00, 417.87it/s]\n",
      "\u001b[34;1mTextFlint\u001b[0m: 5000 in total, 5000 were loaded successful.\n",
      "\u001b[34;1mTextFlint\u001b[0m: ******Finish load!******\n",
      "\u001b[34;1mTextFlint\u001b[0m: ******Start load!******\n",
      "100%|██████████| 500/500 [00:00<00:00, 525.47it/s]\n",
      "\u001b[34;1mTextFlint\u001b[0m: 500 in total, 500 were loaded successful.\n",
      "\u001b[34;1mTextFlint\u001b[0m: ******Finish load!******\n"
     ]
    }
   ],
   "source": [
    "from textflint.input import Dataset\n",
    "# load your data here, we use 5000 train data and 500 test data collected from IMDB dataset\n",
    "train_dataset_path = 'dataset/train.json'\n",
    "test_dataset_path = 'dataset/test.json'\n",
    "\n",
    "# Dataset is defined to use TextFlint  more efficiently\n",
    "train_data = Dataset(task='SA')\n",
    "train_data.load_json(train_dataset_path)\n",
    "\n",
    "test_data = Dataset(task='SA')\n",
    "test_data.load_json(test_dataset_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "179cfea7",
   "metadata": {},
   "source": [
    "## Prepare model and training process"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b6da753",
   "metadata": {},
   "source": [
    "Here we take TextCNN as target model, you should define your target model with pytorch or tensorflow and provide training process. \n",
    "\n",
    "In order to make the model more effectively used by TextFlint, you need to wrapper your model with FlintModel, some functions such as `__call__` and `encode` need to be implemented. More details are available in https://github.com/textflint/textflint/blob/master/textflint/input/model/flint_model/flint_model.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9e24531d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/xin_zhou/anaconda3/envs/demo/lib/python3.7/site-packages/gensim/similarities/__init__.py:15: UserWarning: The gensim.similarities.levenshtein submodule is disabled, because the optional Levenshtein package <https://pypi.org/project/python-Levenshtein/> is unavailable. Install Levenhstein (e.g. `pip install python-Levenshtein`) to suppress this warning.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9cf365667d2a489c941c4cde765f206a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=30.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import os\n",
    "import numpy as np\n",
    "from tqdm.notebook import tqdm\n",
    "from torch.nn import functional as F\n",
    "from textflint.common.utils import logger\n",
    "from textflint.input.model import FlintModel\n",
    "from textflint.input.model.test_model.textcnn_torch_model import TextCNNTorchModel\n",
    "from textflint.input.model.tokenizers.glove_tokenizer import  GloveTokenizer\n",
    "from textflint.input.model.test_model.glove_embedding import  GloveEmbedding\n",
    "from textflint.input.model.test_model.model_helper import *\n",
    "\n",
    "class CNNFlint(FlintModel):\n",
    "    r\"\"\"\n",
    "    Model wrapper for TextCnn implemented by pytorch.\n",
    "\n",
    "    \"\"\"\n",
    "    def __init__(self, model, tokenizer, label2id):\n",
    "        super().__init__(\n",
    "            model=model,\n",
    "            task='SA',\n",
    "            tokenizer=tokenizer\n",
    "        )\n",
    "        self.label2id = label2id\n",
    "\n",
    "    def __call__(self, batch_texts):\n",
    "        r\"\"\"\n",
    "        Tokenize text, convert tokens to id and run the model.\n",
    "\n",
    "        :param batch_texts: (batch_size,) batch text input\n",
    "        :return: numpy.array()\n",
    "\n",
    "        \"\"\"\n",
    "        model_device = next(self.model.parameters()).device\n",
    "        inputs_ids = [self.encode(batch_text) for batch_text in batch_texts]\n",
    "        ids = torch.tensor(inputs_ids).to(model_device)\n",
    "\n",
    "        return self.model(ids).detach().cpu().numpy()\n",
    "\n",
    "    def encode(self, inputs):\n",
    "        r\"\"\"\n",
    "        Tokenize inputs and convert it to ids.\n",
    "\n",
    "        :param inputs: model original input\n",
    "        :return: list of inputs ids\n",
    "\n",
    "        \"\"\"\n",
    "        return self.tokenizer.encode(inputs)\n",
    "\n",
    "    def unzip_samples(self, data_samples):\n",
    "        r\"\"\"\n",
    "        Unzip sample to input texts and labels.\n",
    "\n",
    "        :param list[Sample] data_samples: list of Samples\n",
    "        :return: (inputs_text), labels.\n",
    "\n",
    "        \"\"\"\n",
    "        x = []\n",
    "        y = []\n",
    "\n",
    "        for sample in data_samples:\n",
    "            x.append(sample['x'])\n",
    "            y.append(self.label2id[sample['y']])\n",
    "\n",
    "        return [x], y\n",
    "\n",
    "\n",
    "def train(training_data, args):\n",
    "    '''\n",
    "    Train process for TextCNN, create a TextCNN model and train it with training_data and args\n",
    "    '''\n",
    "\n",
    "    model = TextCNNTorchModel(init_embedding=args['embedding'].embedding)\n",
    "    model.train()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=args['lr'])\n",
    "    epoch_bar = tqdm(range(1, args['epoch'] + 1))\n",
    "    for epoch in epoch_bar:\n",
    "        total_loss = 0\n",
    "        steps = 0\n",
    "        corrects = 0\n",
    "        for inputs, labels in train_iter(training_data, args['batch_size'], args['tokenizer'], args['label2id']):\n",
    "            inputs, labels = torch.from_numpy(inputs).to(args['device']), torch.from_numpy(labels).to(args['device'])\n",
    "            optimizer.zero_grad()\n",
    "            logits = model(inputs)\n",
    "\n",
    "            loss = F.cross_entropy(logits, labels)\n",
    "            total_loss += loss.item()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            result = torch.max(logits, 1)[1].view(labels.size())\n",
    "            steps += 1\n",
    "            corrects += (result.data == labels.data).sum()\n",
    "        accuracy = corrects * 100.0 / (args['batch_size']*steps)\n",
    "#         epoch_bar.set_description('epoch {}'.format(epoch))\n",
    "        epoch_bar.set_description('loss({:.2f}) train acc({:.2f})'.format(total_loss, accuracy))\n",
    "#     print(f\"epoch {epoch}\\t loss {total_loss} train acc {accuracy}\")\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "# Essential parts(embedding, tokenizer) and args for training TextCNN\n",
    "glove_embedding = GloveEmbedding()\n",
    "tokenizer = GloveTokenizer(\n",
    "                    word_id_map=glove_embedding.word2id,\n",
    "                    unk_token_id=glove_embedding.oovid,\n",
    "                    pad_token_id=glove_embedding.padid,\n",
    "                    max_length=512\n",
    "                )\n",
    "label2id = {\"0\": 0, \"1\": 1}\n",
    "train_args = {'lr': 0.0001, 'epoch': 30, 'batch_size': 64, 'device': 'cuda:0', 'max_length': 512, 'label2id':label2id, 'tokenizer':tokenizer, 'embedding':glove_embedding}\n",
    "cnn_model = train(training_data=train_data.dump(), args=train_args)\n",
    "\n",
    "# Wrapper trained model with CNNFlint\n",
    "cnn_flint = CNNFlint(model=cnn_model, tokenizer=tokenizer, label2id=label2id)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e4e153d",
   "metadata": {},
   "source": [
    "## Transform data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b65ad54",
   "metadata": {},
   "source": [
    "We use TextFint to generate new testset to evaluate the robustness of model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5c1ae72c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34;1mTextFlint\u001b[0m: ******Start load!******\n",
      "100%|██████████| 5000/5000 [00:06<00:00, 744.43it/s]\n",
      "\u001b[34;1mTextFlint\u001b[0m: 5000 in total, 5000 were loaded successful.\n",
      "\u001b[34;1mTextFlint\u001b[0m: ******Finish load!******\n",
      "\u001b[34;1mTextFlint\u001b[0m: ******Start DoubleDenial!******\n",
      "100%|██████████| 5000/5000 [01:51<00:00, 45.03it/s]\n",
      "\u001b[34;1mTextFlint\u001b[0m: DoubleDenial, original 5000 samples, transform 4606 samples!\n",
      "\u001b[34;1mTextFlint\u001b[0m: Save samples to ./train_results/ori_DoubleDenial_4606.json!\n",
      "\u001b[34;1mTextFlint\u001b[0m: Save samples to ./train_results/trans_DoubleDenial_4606.json!\n",
      "\u001b[34;1mTextFlint\u001b[0m: ******Finish DoubleDenial!******\n",
      "\u001b[34;1mTextFlint\u001b[0m: ******Start Ocr!******\n",
      "100%|██████████| 5000/5000 [00:33<00:00, 150.99it/s]\n",
      "\u001b[34;1mTextFlint\u001b[0m: Ocr, original 5000 samples, transform 5000 samples!\n",
      "\u001b[34;1mTextFlint\u001b[0m: Save samples to ./train_results/ori_Ocr_5000.json!\n",
      "\u001b[34;1mTextFlint\u001b[0m: Save samples to ./train_results/trans_Ocr_5000.json!\n",
      "\u001b[34;1mTextFlint\u001b[0m: ******Finish Ocr!******\n",
      "\u001b[34;1mTextFlint\u001b[0m: ******Start load!******\n",
      "100%|██████████| 500/500 [00:00<00:00, 772.28it/s]\n",
      "\u001b[34;1mTextFlint\u001b[0m: 500 in total, 500 were loaded successful.\n",
      "\u001b[34;1mTextFlint\u001b[0m: ******Finish load!******\n",
      "\u001b[34;1mTextFlint\u001b[0m: ******Start DoubleDenial!******\n",
      "100%|██████████| 500/500 [00:10<00:00, 48.22it/s]\n",
      "\u001b[34;1mTextFlint\u001b[0m: DoubleDenial, original 500 samples, transform 461 samples!\n",
      "\u001b[34;1mTextFlint\u001b[0m: Save samples to ./test_results/ori_DoubleDenial_461.json!\n",
      "\u001b[34;1mTextFlint\u001b[0m: Save samples to ./test_results/trans_DoubleDenial_461.json!\n",
      "\u001b[34;1mTextFlint\u001b[0m: ******Finish DoubleDenial!******\n",
      "\u001b[34;1mTextFlint\u001b[0m: ******Start Ocr!******\n",
      "100%|██████████| 500/500 [00:03<00:00, 155.07it/s]\n",
      "\u001b[34;1mTextFlint\u001b[0m: Ocr, original 500 samples, transform 500 samples!\n",
      "\u001b[34;1mTextFlint\u001b[0m: Save samples to ./test_results/ori_Ocr_500.json!\n",
      "\u001b[34;1mTextFlint\u001b[0m: Save samples to ./test_results/trans_Ocr_500.json!\n",
      "\u001b[34;1mTextFlint\u001b[0m: ******Finish Ocr!******\n"
     ]
    }
   ],
   "source": [
    "from textflint.engine import Engine\n",
    "from textflint.adapter import auto_config\n",
    "from textflint.input.config import Config\n",
    "from textflint.common.settings import UNMATCH_UT_TRANSFORMATIONS, \\\n",
    "    TASK_TRANSFORMATIONS\n",
    "\n",
    "# Config for TextFlint to transform data, see more details in https://textflint.readthedocs.io/en/latest/\n",
    "config = auto_config(task='SA')\n",
    "config.trans_methods = ['DoubleDenial', 'Ocr']\n",
    "config.sub_methods = []\n",
    "engine = Engine()\n",
    "# Generate adversarial train data, saved in ./train_results dir\n",
    "config.out_dir = './train_results'\n",
    "engine.run(data_input=train_dataset_path, config=config)\n",
    "\n",
    "# Generate adversarial test data, saved in ./test_results dir\n",
    "config.out_dir = './test_results'\n",
    "engine.run(data_input=test_dataset_path, config=config)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9513fbe2",
   "metadata": {},
   "source": [
    "## Evaluate robustness of model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee22f772",
   "metadata": {},
   "source": [
    "Next step we using transformed test data to evaluate the performance of original TextCNN model. You can see that the performance of model is reduced on the transformed sample. This proves that the robustness of TextCNN is not very good. It's a very common phenomenon, not just in the TextCNN model. But adversarial training is useful to alleviate this problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "851779c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34;1mTextFlint\u001b[0m: ******Start load!******\n",
      "100%|██████████| 461/461 [00:00<00:00, 742.60it/s]\n",
      "\u001b[34;1mTextFlint\u001b[0m: 461 in total, 461 were loaded successful.\n",
      "\u001b[34;1mTextFlint\u001b[0m: ******Finish load!******\n",
      "\u001b[34;1mTextFlint\u001b[0m: ******Start load!******\n",
      "100%|██████████| 461/461 [00:00<00:00, 702.36it/s]\n",
      "\u001b[34;1mTextFlint\u001b[0m: 461 in total, 461 were loaded successful.\n",
      "\u001b[34;1mTextFlint\u001b[0m: ******Finish load!******\n",
      "\u001b[34;1mTextFlint\u001b[0m: ******Start load!******\n",
      "100%|██████████| 500/500 [00:00<00:00, 699.62it/s]\n",
      "\u001b[34;1mTextFlint\u001b[0m: 500 in total, 500 were loaded successful.\n",
      "\u001b[34;1mTextFlint\u001b[0m: ******Finish load!******\n",
      "\u001b[34;1mTextFlint\u001b[0m: ******Start load!******\n",
      "100%|██████████| 500/500 [00:00<00:00, 563.69it/s]\n",
      "\u001b[34;1mTextFlint\u001b[0m: 500 in total, 500 were loaded successful.\n",
      "\u001b[34;1mTextFlint\u001b[0m: ******Finish load!******\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[({'DoubleDenial_ori_accuracy': 0.8459869848156182},\n",
       "  {'DoubleDenial_trans_accuracy': 0.665943600867679}),\n",
       " ({'Ocr_ori_accuracy': 0.844}, {'Ocr_trans_accuracy': 0.84})]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "def find_target_data(trans_name, output_dir, prefix='trans'):\n",
    "    '''\n",
    "    Find target data generated by TextFlint in output_dir\n",
    "    '''\n",
    "    file_list = os.listdir(output_dir)\n",
    "    for file in file_list:\n",
    "        if re.match('{}_{}_\\d+.json'.format(prefix, trans_name), file):\n",
    "            return '{}/{}'.format(output_dir, file)\n",
    "    return None\n",
    "\n",
    "def compare_evaluate(flintmodel, trans_name, output_dir='./test_results'):\n",
    "    '''\n",
    "    Evaluate flintmodel in original test data and transformed test data\n",
    "    '''\n",
    "    ori_test_data = Dataset(task='SA')\n",
    "    ori_test_data.load_json(find_target_data(trans_name=trans_name, output_dir=output_dir, prefix='ori'))\n",
    "    trans_test_data = Dataset(task='SA')\n",
    "    trans_test_data.load_json(find_target_data(trans_name=trans_name, output_dir=output_dir, prefix='trans'))\n",
    "\n",
    "    flintmodel.model.eval()\n",
    "    ori_eval_result = flintmodel.evaluate(ori_test_data.dump(), prefix='{}_ori_'.format(trans_name))\n",
    "    trans_eval_result = flintmodel.evaluate(trans_test_data.dump(), prefix='{}_trans_'.format(trans_name))\n",
    "    return ori_eval_result, trans_eval_result\n",
    "\n",
    "ori_model_results = []\n",
    "\n",
    "# Evaluate cnn_flint in original test data and transformed test data.\n",
    "for trans_method in config.trans_methods:\n",
    "#     print('Evaluate the performence of TextCNN {}'.format(trans_method))\n",
    "    cnn_results = compare_evaluate(cnn_flint, trans_method)\n",
    "    ori_model_results.append(cnn_results)\n",
    "#     print('Adversarial training for TextCNN {}'.format(trans_method))\n",
    "\n",
    "ori_model_results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a5eafde",
   "metadata": {},
   "source": [
    "## Adversarial training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bd76a10",
   "metadata": {},
   "source": [
    "To enhance model's robustness, we add the adversarial sample to original dataset and re-train the model. (There are lots of ways to do adversarial training, we choose the most common one). We found that the performance of the model with adversarial training improved significantly comparing to the same model without adversarial training as we expected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c64199ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34;1mTextFlint\u001b[0m: ******Start load!******\n",
      "100%|██████████| 4606/4606 [00:07<00:00, 621.87it/s]\n",
      "\u001b[34;1mTextFlint\u001b[0m: 4606 in total, 4606 were loaded successful.\n",
      "\u001b[34;1mTextFlint\u001b[0m: ******Finish load!******\n",
      "\u001b[34;1mTextFlint\u001b[0m: ******Start load!******\n",
      "100%|██████████| 5000/5000 [00:06<00:00, 762.59it/s]\n",
      "\u001b[34;1mTextFlint\u001b[0m: 5000 in total, 5000 were loaded successful.\n",
      "\u001b[34;1mTextFlint\u001b[0m: ******Finish load!******\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5e4838b370b744e38555ae8a16b67108",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=30.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34;1mTextFlint\u001b[0m: ******Start load!******\n",
      " 14%|█▎        | 63/461 [00:00<00:00, 620.68it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 461/461 [00:00<00:00, 741.02it/s]\n",
      "\u001b[34;1mTextFlint\u001b[0m: 461 in total, 461 were loaded successful.\n",
      "\u001b[34;1mTextFlint\u001b[0m: ******Finish load!******\n",
      "\u001b[34;1mTextFlint\u001b[0m: ******Start load!******\n",
      "100%|██████████| 461/461 [00:00<00:00, 765.22it/s]\n",
      "\u001b[34;1mTextFlint\u001b[0m: 461 in total, 461 were loaded successful.\n",
      "\u001b[34;1mTextFlint\u001b[0m: ******Finish load!******\n",
      "\u001b[34;1mTextFlint\u001b[0m: ******Start load!******\n",
      "100%|██████████| 5000/5000 [00:07<00:00, 630.18it/s]\n",
      "\u001b[34;1mTextFlint\u001b[0m: 5000 in total, 5000 were loaded successful.\n",
      "\u001b[34;1mTextFlint\u001b[0m: ******Finish load!******\n",
      "\u001b[34;1mTextFlint\u001b[0m: ******Start load!******\n",
      "100%|██████████| 5000/5000 [00:06<00:00, 761.84it/s]\n",
      "\u001b[34;1mTextFlint\u001b[0m: 5000 in total, 5000 were loaded successful.\n",
      "\u001b[34;1mTextFlint\u001b[0m: ******Finish load!******\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3685a3984b784fe5b86b2fd8fe9a0bff",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=30.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34;1mTextFlint\u001b[0m: ******Start load!******\n",
      " 15%|█▌        | 75/500 [00:00<00:00, 749.95it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 500/500 [00:00<00:00, 767.24it/s]\n",
      "\u001b[34;1mTextFlint\u001b[0m: 500 in total, 500 were loaded successful.\n",
      "\u001b[34;1mTextFlint\u001b[0m: ******Finish load!******\n",
      "\u001b[34;1mTextFlint\u001b[0m: ******Start load!******\n",
      "100%|██████████| 500/500 [00:00<00:00, 740.54it/s]\n",
      "\u001b[34;1mTextFlint\u001b[0m: 500 in total, 500 were loaded successful.\n",
      "\u001b[34;1mTextFlint\u001b[0m: ******Finish load!******\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------Original TextCNN model results-----------------\n",
      "[({'DoubleDenial_ori_accuracy': 0.8459869848156182}, {'DoubleDenial_trans_accuracy': 0.665943600867679}), ({'Ocr_ori_accuracy': 0.844}, {'Ocr_trans_accuracy': 0.84})]\n",
      "-----------------TextCNN with adversarial training results-----------------\n",
      "[({'DoubleDenial_ori_accuracy': 0.8568329718004338}, {'DoubleDenial_trans_accuracy': 0.8459869848156182}), ({'Ocr_ori_accuracy': 0.866}, {'Ocr_trans_accuracy': 0.862})]\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "def adv_train(trans_name, output_dir='./train_results'):\n",
    "    '''\n",
    "    Adversarial training for TextCNN and target transformation\n",
    "    '''\n",
    "    # Mix up original training data and adversarial sample\n",
    "    mix_train_data = Dataset(task='SA')\n",
    "    mix_train_data.load_json(find_target_data(trans_name=trans_name, output_dir=output_dir))\n",
    "    mix_train_data.load_json(train_dataset_path)\n",
    "    \n",
    "    # retrain TextCNN with mixed data and evaluate it in original test data and transformed test data\n",
    "    adv_cnn_model = train(training_data=mix_train_data.dump(), args=train_args)\n",
    "    adv_cnn_flint = CNNFlint(model=adv_cnn_model, tokenizer=tokenizer, label2id=label2id)\n",
    "    return adv_cnn_flint\n",
    "\n",
    "adv_model_results = []\n",
    "\n",
    "# adversarial training for cnn_flint, and evaluate new model in original test data and transformed data.\n",
    "for trans_method in config.trans_methods:\n",
    "    adv_cnn_flint = adv_train(trans_method)\n",
    "    adv_cnn_result = compare_evaluate(adv_cnn_flint, trans_method)\n",
    "    adv_model_results.append(adv_cnn_result)\n",
    "\n",
    "# Compare the performance of original model and adversarial training model\n",
    "print('-----------------Original TextCNN model results-----------------')\n",
    "print(ori_model_results)\n",
    "print('-----------------TextCNN with adversarial training results-----------------')\n",
    "print(adv_model_results)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
